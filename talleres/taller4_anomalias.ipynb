{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BK9dmG7MSOya"
   },
   "source": [
    "#  <center> Taller  de Aprendizaje Automático </center>\n",
    "##  <center> Taller 4: Detección de Anomalías  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PjU4ItOTaGr"
   },
   "source": [
    "En la siguiente actividad se trabajará en la detección de anomalías sobre redes de computadoras a partir de datos de tráfico. Para esto se utilizará una parte del conjunto [KDD Cup'99](https://scikit-learn.org/stable/datasets/real_world.html#kddcup99-dataset) pensada para evaluar métodos de detección de anomalías. \n",
    "\n",
    "Para los problemas de detección de anomalías generalmente no se cuenta con datos etiquetados para entrenar un detector. Por su definición las anomalías son eventos raros y por lo tanto poco frecuentes, lo que dificulta el etiquetado. Es por esto que este tipo de tareas generalmente son no supervisadas.\n",
    "\n",
    "El enfoque más habitual para implementar soluciones para este tipo de problemas, es crear un modelo base a partir de un conjunto de datos \"normales\", es decir de los cuales se tenga cierta certeza de que todos fueron adquiridos en una situación normal. Luego en producción se detectarán como datos anómalos todos aquellos que no se ajusten a este modelo. Para saber el grado de ajuste de los datos se debe seleccionar un punto de operación, es decir, determinar cuándo un dato se considera anómalo. En un ejemplo real, el cliente primero debería proporcionar una cantidad considerable de datos que representen el comportamiento normal de su sistema. Luego que se tiene el mejor modelo posible de estos datos, junto con el cliente, que es el que conoce su sistema, se debe determinar el punto de operación a partir del compromiso entre detectar la mayor cantidad de anomalías y obtener la menor cantidad de falsas alarmas posibles.\n",
    "\n",
    "Para hacer investigación en la detección de anomalías, existen conjuntos de datos como el que se trabajará en esta actividad que si tienen etiquetas. Generalmente estas se obtienen provocando fallas y/o ataques intencionales a un sistema que se encuentra funcionando de manera normal. En esta actividad se separará el conjunto de entrenamiento en dos partes. La primera con una gran proporción de datos etiquetados como normales, simulará ser el conjunto que el cliente nos proporciona para entrenar nuestro modelo. El otro conjunto tendrá datos etiquetados como normales o como anómalos, que se utilizará para definir el punto de operación. Luego, se descargarán los datos de test asociados a este problema para evaluar la puesta en producción del modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnXnqw5KS3zd"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "\n",
    "*   Abordar un problema de detección de anomalías, y ver las diferencias con un problema de clasificación convencional.\n",
    "*   Trabajar con algoritmos de aprendizaje no supervisado.\n",
    "*   Crear detectores compatibles con los *pipelines* de *scikit-learn*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "common-destiny"
   },
   "source": [
    "# Formas de trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "moral-gallery"
   },
   "source": [
    "### Opción 1: Trabajar localmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Descargar los datos en su máquina personal y trabajar en su propio ambiente de desarrollo.\n",
    "\n",
    "`conda activate TAA-py38`              \n",
    "`jupyter-notebook`    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Los paquetes faltantes se pueden instalar desde el notebook haciendo:     \n",
    "` !pip install paquete_faltante` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lined-sport"
   },
   "source": [
    "### Opción 2:  Trabajar en *Colab*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lined-candle"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/TAA-fing/TAA-2022/blob/main/talleres/taller4_anomalias.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Ejecutar en Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "expensive-jewel"
   },
   "source": [
    "Se puede trabajar en Google Colab. Para ello es necesario contar con una cuenta de **google drive** y ejecutar un notebook almacenado en dicha cuenta. De lo contrario, no se conservarán los cambios realizados en la sesión. En caso de ya contar con una cuenta, se puede abrir el notebook y luego ir a `Archivo-->Guardar una copia en drive`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRtWLunGoohY"
   },
   "source": [
    "# Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1 - Levantar los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjunto de Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El subconjunto de [KDD Cup'99](https://scikit-learn.org/stable/datasets/real_world.html#kddcup99-dataset) con el que se trabajará contiene 100655 instancias donde cada una cuenta con 41 características. Por más información sobre el contenido de las características haga clic [aquí](http://kdd.ics.uci.edu/databases/kddcup99/task.html). Además, se cuenta con la columna *'labels'* que indica si el dato es normal o, de no serlo, indica el tipo de anomalía.\n",
    "\n",
    "#### Objetivos:\n",
    "\n",
    " - Levantar el conjunto de datos de entrenamiento. \n",
    " - Determinar la relación entre datos normales y anómalos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cuZHaZIlpyMh",
    "outputId": "0a7d02c1-c6b9-424a-a1f2-39be4b1c09e3"
   },
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1GhXE4acp036",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "\n",
    "#Se obtienen los datos en formato de diccionario\n",
    "KDDSA = fetch_kddcup99(subset='SA', as_frame=True, )\n",
    "\n",
    "#A partir del diccionario se crea un Dataframe con los datos\n",
    "df = pd.DataFrame(data=KDDSA.frame.values, columns=KDDSA.frame.columns)\n",
    "\n",
    "#Se estandariza el formato de los datos en el Dataframe \n",
    "types = [float, str, str,str, float, float, str, float, float, float, float, str, float, float,float, float, float, float, float, float, str, str, \n",
    "         float, float, float, float,float, float, float, float,float, float, float, float, float, float, float,float, float, float, float, str]\n",
    "\n",
    "columns = df.columns\n",
    "for i in range(len(columns)):\n",
    "    df[columns[i]] = df[columns[i]].astype(types[i])\n",
    "    if types[i] == str:\n",
    "        df[columns[i]]= df[columns[i]].str.replace(\"b'\", \"\")\n",
    "        df[columns[i]]= df[columns[i]].str.replace(\"'\", \"\")\n",
    "\n",
    "#Visualizo\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjunto de Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este problema tiene disponible un conjunto de datos para Test, estos se pueden encontar [aquí](https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html). \n",
    "\n",
    "#### Objetivos:\n",
    "\n",
    " - Descargar y levantar los datos de Test. \n",
    " - Separar los datos de las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "#Descargo los datos de Test\n",
    "wget.download('http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Levanto los datos de Test\n",
    "test_data = pd.read_csv('./corrected.gz', header=None)\n",
    "\n",
    "test_data.columns = columns\n",
    "\n",
    "#Se estandariza el formato de los datos en el Dataframe \n",
    "types = [float, str, str,str, float, float, str, float, float, float, float, str, float, float,float, float, float, float, float, float, str, str, \n",
    "         float, float, float, float,float, float, float, float,float, float, float, float, float, float, float,float, float, float, float, str]\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    test_data[columns[i]] = test_data[columns[i]].astype(types[i])\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Parte 2 - Preparar Conjuntos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Asumiendo que se mantuvo el orden en el que fueron descargados los datos:\n",
    "\n",
    "#### Objetivos:\n",
    "\n",
    " - Separar los primeros 90000 datos de entrenamiento y sus respectivas etiquetas para el entrenamiento de los modelos. Estos deberían estar etiquetados como normales. \n",
    " - Los restantes datos del conjunto de entrenamiento se utilizaran para validación.  Este conjunto permitirá fijar el punto de operación del modelo.\n",
    " - Cambie las etiquetas de los datos en todos los conjuntos a 0 y 1. Siendo 0 la etiqueta de la clase normal y 1 la etiqueta de la clase anómala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Parte 3 - Preprocesar y Visualizar los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Objetivos:\n",
    "\n",
    " - Analizar y visualizar los datos. \n",
    " - Realizar un **pipeline** que realice los los preprocesamientos que considere necesarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAXVfNNz0DZe"
   },
   "source": [
    "# Detección de anomalías\n",
    "\n",
    "Si bien *scikit-learn* proporciona algunas herramientas para la detección de anomalías, esta actividad se centra en la utilización de algoritmos no supervisados generalmente pensados para otras tareas como: reducción de la dimensionalidad, y/o *clustering*. Específicamente se trabajará con: PCA, *K-Means*, y *Gaussian Mixture Model* (GMM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "rBVPxdE27wvI"
   },
   "source": [
    "## PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Parte 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Objetivos:\n",
    "\n",
    " - Aplicar PCA sobre los datos de entrenamiento y graficar como varía el porcentaje de la varianza total en función de la cantidad de componentes principales (CPs). Se sugiere ver la sección *Choosing the Right Number of Dimensions* del capítulo 8 del libro. \n",
    " - Determinar la cantidad de CPs de manera de mantener el *99%* de la varianza de los datos.\n",
    " - Utilizando PCA visualice los datos en dos o tres dimensiones. ¿Logra identificar clusters? *Se sugiere utilizar colores distintos para los puntos Normales y Anómales*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "La forma más directa de hacer detección de anomalías utilizando PCA, es mediante el error de reconstrucción. Para esto primero se calculan los componentes principales a partir de los datos reservados para el modelado. Luego, para cada dato a analizar se lo proyecta sobre estos componentes, y se calcula su reconstrucción. Debido a que los CPs fueron calculado sólo con datos normales, se espera que la reconstrucción de un dato anómalos tenga grandes errores. Es por esto que a partir del error de reconstrucción se pueda determinar si un dato es anómalo o no.\n",
    "\n",
    "#### Objetivos:\n",
    "\n",
    "*   Implementar un detector tal como se describe arriba, utilizando RMSE para calcular el error. El mismo se debe definir como una clase de manera que sea compatible con los *pipelines* de *scikit-learn*. En la siguiente celda se muestra un *template* para crear la clase ([aquí](https://scikit-learn.org/stable/developers/develop.html) se pueden ver otros ejemplos).\n",
    "\n",
    "*   Crear un *pipeline* que incluya el preprocesamiento y el detector implementado.\n",
    "*   Entrenar el modelo de manera que mantenga el *99%* de la varianza. \n",
    "*   Proponga un punto de operación teniendo en cuenta que se quiere evitar un exceso de falsas alarmas. Para ello se recomienda graficar el compromiso entre *precision* y *recall* para distintos valores de *threshold* que definen el punto de operación. Ver la sección *Precision/Recall Trade-off* del capítulo 3 del libro.\n",
    "*   Graficar los *scores* de los datos utilizados en el punto anterior, diferenciando con colores los datos normales de los anómalos.\n",
    "*    Evaluar el desempeño en el conjunto de Validación y Test. Puede resultar útil realizar una matriz de confusión. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, OutlierMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class AD_PCA(BaseEstimator, OutlierMixin):\n",
    "    def __init__(self, n_comp=None):\n",
    "        self.n_comp = n_comp\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y \n",
    "        self.PCA_ = PCA(n_components=self.n_comp)\n",
    "        self.PCA_.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        X = check_array(X)\n",
    "        check_is_fitted(self, ['X', 'y'])\n",
    "\n",
    "        # Agregar código---\n",
    "\n",
    "        #------------------\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "7zI3BtzycQIB"
   },
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Parte 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Objetivos:\n",
    "\n",
    " - Proponer una forma de hallar la cantidad óptima de clusters. Se sugiere ver la sección *Finding the optimal number of clusters* del Capítulo 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Parte 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "XnYteDcOxmr0"
   },
   "source": [
    "Utilizando la cantidad óptima de cluster hallada:\n",
    "\n",
    "#### Objetivos:\n",
    "\n",
    "- Implementar un detector de anomalías utilizando *K-Means*. Para ello cree una clase y un *pipeline* que lo implemente de forma análoga a lo realizado con el método de PCA.\n",
    "-   Proponga un punto de operación teniendo en cuenta el compromiso entre *precision* y *recall* para distintos valores de *threshold* que definen el punto de operación. \n",
    "-   Graficar los *scores* de los datos utilizados en el punto anterior, diferenciando con colores los datos normales de los anómalos. \n",
    "- Evaluar el desempeño en el conjunto de Validación y Test. Puede resultar útil realizar una matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, OutlierMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "class AD_Kmeans(BaseEstimator, OutlierMixin):\n",
    "    def __init__(self, n_clusters=8):\n",
    "        self.K = n_clusters\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y \n",
    "        self.Kmeans_ = KMeans(n_clusters=self.K)\n",
    "        self.Kmeans_.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        X = check_array(X)\n",
    "        check_is_fitted(self, ['X', 'y'])\n",
    "\n",
    "         # Agregar código---\n",
    "\n",
    "        #------------------\n",
    "    \n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "U2d6LJtglvN2"
   },
   "source": [
    "## Gaussian Mixtures Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Parte 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "RFpfYG1Q7-mJ"
   },
   "source": [
    "Siguiendo el ejemplo de la sección *Anomaly Detection Using Gaussian Mixture* en el Capítulo 9 del libro. \n",
    "\n",
    "#### Objetivos:\n",
    "\n",
    " - Proponer una forma de hallar la cantidad óptima de mezclas a utilizar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "#### Objetivos:\n",
    "\n",
    "*   Implementar un detector que calcule el valor de los *scores*, y que además determine las predicciones a partir de un *threshold* calculado de manera similar al ejemplo del Capítulo 9. En la siguiente celda se proporciona un *template* para la implementación del detector.\n",
    "*   Obtener diferentes predicciones para los datos del punto de operación,variando el parámetro *percen*. Discutir sobre los resultados.\n",
    "- Evaluar el desempeño en el conjunto de Validación y Test. Puede resultar útil realizar una matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true,
    "id": "qErVm-2IpLH2"
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.base import BaseEstimator, OutlierMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true,
    "id": "dxwCxHIGcocg"
   },
   "outputs": [],
   "source": [
    "class AD_GMM(BaseEstimator, OutlierMixin):\n",
    "\n",
    "    def __init__(self, n_components=1, percen=0):\n",
    "        self.n_comp = n_components\n",
    "        self.percen = percen\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.classes_ = [1, 0]\n",
    "        self.X = X\n",
    "        self.y = y \n",
    "        self.GMM_ = GaussianMixture(n_components=self.n_comp)\n",
    "        self.GMM_.fit(X)\n",
    "\n",
    "        #threshold---\n",
    "\n",
    "        #------------\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        X = check_array(X)\n",
    "        check_is_fitted(self, ['X', 'y'])\n",
    "\n",
    "        # Agregar código---\n",
    "\n",
    "        #------------------\n",
    "        return score\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = check_array(X)\n",
    "        check_is_fitted(self, ['X', 'y'])\n",
    "\n",
    "        # Agregar código---\n",
    "\n",
    "        #------------------\n",
    "        return pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoNvBYnaqyqT"
   },
   "source": [
    "# Opcional\n",
    "\n",
    "*   Aplicar a los datos del problema alguno de los detectores de *sikit-learn* como: One-Class SVM, Isolation Forest\n",
    "*   Comparar con los detectores anteriores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobre el Proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto de Operación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El desempeño del modelo desarrollado para resolver el problema de [detección del bosón de Higgs](https://www.kaggle.com/c/higgs-boson) se evaluará utilizando la métrica *AMS* que se define como: \n",
    "\n",
    "$$\n",
    "\\mathrm{AMS}=\\sqrt{2\\left(\\left(s+b+b_{r}\\right) \\log \\left(1+\\frac{s}{b+b_{r}}\\right)-s\\right)}\n",
    "$$\n",
    "\n",
    "Donde: \n",
    "\n",
    "- $s$, $b$ : tasas de verdaderos positivos y falsos positivos no normalizados respectivamente.\n",
    "- $b_r =10$ es el término de regularización. \n",
    "- $\\log$ representa el logaritmo natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define el conjunto de datos $\\mathcal{D}=\\left\\{\\mathbf{x}_{\\mathbf{i}}, y_{i}, w_{i}\\right\\}$ siendo $\\mathbf{x}_{\\mathbf{i}}$ las características físicas del experimento (medidas primitivas y derivadas), $y_{i} \\in\\{b, s\\}$ el resultado del evento y $w_{i}$ el peso. Se denominan los subconjuntos de índices $\\mathcal{S}=\\left\\{i: y_{i}=s\\right\\}$ y $\\mathcal{B}=\\left\\{i: y_{i}=b\\right\\}$.\n",
    "\n",
    "El cálculo de $s$ y $b$ se obtienen como: \n",
    "\n",
    "$$\n",
    "\\mathbf{s}=\\sum_{i \\in \\mathcal{S} \\cap \\mathcal{G}} w_{i} \\quad \\mathbf{b}=\\sum_{i \\in \\mathcal{B} \\cap \\mathcal{G}} w_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde se denota $\\mathcal{G}=\\left\\{i: \\hat{y}_{i}=s\\right\\}$, siendo $\\hat{y}_{i}$ la predicción del modelo utilizado. Observar que $s$ y $b$ son las tasas de verdaderos y falsos positivos respectivamente como se mencionó anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargue la implementación de la métrica de la página del [curso](https://eva.fing.edu.uy/mod/resource/view.php?id=135850). Modifique el nombre del archivo descargado a *HiggsBosonUtils.py* y guarde dicho archivo en una carpeta *tools* en el mismo directorio donde está el presente Notebook. En estas condiciones, puede importar la función *AMS* contenida dentro de *HiggsBosonUtils.py* de la siguiente forma:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.HiggsBosonUtils import AMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 100\n",
    "b = 1000000\n",
    "AMS(s,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que como se trabajó en el presente taller, durante el proyecto será necesario también fijar un punto de operación que maximice el desempeño de la métrica, en este caso, el *AMS*. En función de ello, resulta pertinente observar que valores resultan razonables para dicha métrica. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objetivos:\n",
    "\n",
    " - Levantar el conjunto de datos del proyecto. \n",
    " - Obtener el desempeño de *AMS* obtenido si el modelo clasifica todos los eventos como *background*.\n",
    " - Obtener el desempeño de *AMS* obtenido si el modelo clasifica todos los eventos como *signal*.\n",
    " - Obtener el desempeño de *AMS* obtenido si el modelo clasifica perfectamente todos los eventos.\n",
    " - Observe los Leaderboard de Kaggle y en base a los resultados obtenidos anteriormente que discuta que valores le resultan razonables de *AMS*.\n",
    " - Con los datos del proyecto genere dos conjuntos uno de Entrenamiento y otro de Validación. Luego, entrene el modelo y evalúe el desempeño en el conjunto de Validación. Observe como varía el AMS al modificar el umbral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Taller4_detección_de_anomalías.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
