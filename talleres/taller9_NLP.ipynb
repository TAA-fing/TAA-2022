{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qjsf78ymkS0E"
   },
   "source": [
    "#  <center> Taller  de Aprendizaje Automático </center>\n",
    "##  <center> Taller 9: *Natural Language Processing* (NLP)  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUfgw7Siq3td"
   },
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4y3HbLOkOVD"
   },
   "source": [
    "La siguiente actividad propone el abordaje de un problema de procesamiento de lenguaje natural (NLP) utilizando herramientas de *embedding* y modelos RNN. El conjunto de datos que se utilizará es IMDb, el cual corresponde a un problema de clasificación donde se tienen 50000 criticas de películas (35000 de *train* y 15000 de *test*), y se quiere estimar si éstas son críticas positivas (1) o negativas (0). \n",
    "\n",
    "La propuesta consiste en entender y reproducir los pasos de la sección *Sentiment Analysis* para los datos **sin procesar**, agregando algunas variantes como mitigar el sobreajuste y entender la herramienta *embeddings*.\n",
    "\n",
    "En este Taller también se introduce la biblioteca *Streamlit*, utilizada para desarrollar prototipos de aplicaciones web de aprendizaje automático. Aquellos que así lo deseen, podrán generar de manera sencilla una aplicación web que clasifique las críticas proporcionadas por los usuarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rY6vz2Ekj8ig"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "\n",
    "*   Aplicar modelos basados en RNN a un problema de NLP.\n",
    "*   Trabajar con embeddings para secuencias de texto, en particular embeddings preentrenados.\n",
    "*   Utilizar herramientas para la visualización de embeddings.\n",
    "*  (Opcional, no evaluado) Desarrollar una aplicación web que clasifique críticas proporcionadas por los usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "common-destiny"
   },
   "source": [
    "## Formas de trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVgxoLgl1-KA"
   },
   "source": [
    "### Opción 1: Trabajar localmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moral-gallery"
   },
   "source": [
    "\n",
    "Descargar los datos en su máquina personal y trabajar en su propio ambiente de desarrollo.\n",
    "\n",
    "`conda activate TAA-py38`              \n",
    "`jupyter-notebook`    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfcTy55R2A7w"
   },
   "source": [
    "Los paquetes faltantes se pueden instalar desde el notebook haciendo:     \n",
    "` !pip install paquete_faltante` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lined-sport"
   },
   "source": [
    "### Opción 2:  Trabajar en *Colab*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lined-candle"
   },
   "source": [
    "<table align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/TAA-fing/TAA-2022/blob/main/talleres/taller9_NLP.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Ejecutar en Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "expensive-jewel"
   },
   "source": [
    "Se puede trabajar en Google Colab. Para ello es necesario contar con una cuenta de **google drive** y ejecutar un notebook almacenado en dicha cuenta. De lo contrario, no se conservarán los cambios realizados en la sesión. En caso de ya contar con una cuenta, se puede abrir el notebook y luego ir a `Archivo-->Guardar una copia en drive`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8OOpzq91ITq"
   },
   "source": [
    "La siguiente celda realiza la configuración necesaria para obtener datos desde la plataforma Kaggle. Le solicitará que suba el archivo *kaggle.json* asociado a su cuenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "LlKwJ6sKmhBt",
    "outputId": "0af08661-a8df-47f2-a5de-b667bf1c2319"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from google.colab import files\n",
    "\n",
    "# El siguiente archivo solicitado es para habilitar la API de Kaggle en el entorno que está trabajando.\n",
    "# Este archivo se descarga entrando a su perfíl de Kaggle, en la sección API, presionando donde dice: Create New API Token\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))\n",
    "\n",
    "#Then move kaggle.json into the folder where the API expects to find it.\n",
    "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj1R6mm14u0t"
   },
   "source": [
    "# Parte 1: Análisis y preprocesamiento de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M44JXP4Z41BI"
   },
   "source": [
    "Se utilizará el conjunto de IMDb provisto por Kaggle. Se tienen 50000 criticas de películas que al igual que en el *Taller 2* se utilizarán 35000 para *train* y 15000 para *test*.\n",
    "\n",
    "*   Ejecutar la siguiente celda para descargar el conjunto y verificar que los conjuntos tienen la cantidad de instancias esperadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CwAZeeqamXo2",
    "outputId": "171a83c3-40f2-4e0d-a80f-f6ae3dff31b7"
   },
   "outputs": [],
   "source": [
    "# Descarga la base IMDb de Kaggle\n",
    "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nj1ZMjmtn3cX",
    "outputId": "ed03d52e-555c-4229-96e6-6e2bcae40596"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Se descomprime el archivo descargado\n",
    "with zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('')\n",
    "\n",
    "# Se levanta como pandas DataFrame\n",
    "data_file = 'IMDB Dataset.csv'\n",
    "data = pd.read_csv(data_file)\n",
    "\n",
    "#Separación de Conjuntos\n",
    "N=35000\n",
    "X_train = data.loc[:N-1, 'review'].values\n",
    "y_train = data.loc[:N-1, 'sentiment'].values == 'positive' \n",
    "X_test = data.loc[N:, 'review'].values\n",
    "y_test = data.loc[N:, 'sentiment'].values == 'positive'\n",
    "\n",
    "# Armado de los Tensorflow Datasets\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "#Verificación\n",
    "print('Train Size:', len(dataset_train),'Test Size:', len(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQpeE5zh8ZlC"
   },
   "source": [
    "*   Del conjunto de entrenamiento visualizar tanto una crítica positiva como una negativa. Se sugiere ir a los Notebooks del *Capítulo 16*. Puede ser útil el uso del método *.skip()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMp1E9d27ro5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYU0vZiD65J1"
   },
   "source": [
    "*   Reservar unas 5000 críticas de los datos de entrenamiento para validación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D55BIKVp7M2K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNVjgj0n8yPP"
   },
   "source": [
    "* Siguiendo el ejemplo del libro genere una función para preprocesar las reseñas, y una función que genere la tabla del vocabulario utilizando solamente los datos de *train*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzuRMOsE9IKh"
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(datasets, batch_size=32, vocab_size=10000, num_oov_buckets=1000):\n",
    "  \n",
    "  '''\n",
    "  Función que dado un tensorflow dataset (datasets) con críticas, calcula el número de ocurrencias \n",
    "  de las palabras y genera un vocabulario con las vocab_size palabras más frecuentes. Una vez generado \n",
    "  ese vocabulario, la función devuelve las palabras del vocabulario (words) con sus respectivos índices \n",
    "  (word_ids) y una tabla de vocabulario que admite num_oov_buckets palabras fuera del vocabulario. \n",
    "\n",
    "  Entrada: \n",
    "\n",
    "    datasets: Tensorflow Dataset con las críticas. \n",
    "    batch_size: Tamaño de los batches a preprocesar.\n",
    "    vocab_size: Tamaño del Vocabulario\n",
    "    num_oov_buckets: Cantidad de palabras fuera del vocabulario (Out-of-Vocabulary bucket)\n",
    "\n",
    "  Salida:\n",
    "    table: Tabla del Vocabulario\n",
    "    words: Palabras del Vocabulario\n",
    "    word_ids: Indices de las palabras en el Vocabulario\n",
    "\n",
    "  '''\n",
    "  \n",
    "  #Solución\n",
    "  \n",
    "  return table, words, word_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8GH029P-XFF"
   },
   "source": [
    "* Genere una función que a partir de la reseña y la tabla del vocabulario genere un vector para entrenar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pk1Umedw-h5U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgEMGtTs-qxq"
   },
   "source": [
    "* Preprocese los conjuntos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVu7kOfb-ulZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eBa6wfN-imc"
   },
   "source": [
    "* ¿Cuáles son las 15 palabras más frecuentes en los datos de *train*? De acuerdo con lo observado, ¿considera que se podría mejorar el preprocesamiento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Rqu3OKJjw2c"
   },
   "source": [
    "Respuesta: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUSqc8-o4Xj2"
   },
   "source": [
    "*   Una vez preprocesados los datos ¿cuáles son los largos de secuencias para los primeros *batches*? ¿Es un problema a resolver que todos tengan largos distintos? ¿Por qué?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRMLPZ2NjzfY"
   },
   "source": [
    "Respuesta: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNZlLb6v4OHS"
   },
   "source": [
    "# Parte 2: *Embedding*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmZeKwMu4dlD"
   },
   "source": [
    "* Cree y entrene el modelo que aparece al final de la sección *Sentiment Analysis* y previo a la sección *Masking* del capítulo 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Zm3X0zu4dGU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CYXdef9GK_D"
   },
   "source": [
    "* Observe que el modelo se sobreajusta a los datos. Utilice alguna técnica vista para regularizar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OhsylbEj988"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqU7n4gMZc5X"
   },
   "source": [
    "* El modelo cuenta con una capa de entrada de *embedding* la cual abarca la mayoría de los parámetros entrenables. En este caso un *embedding* es un vector entrenable que representa una palabra en nuevo espacio cuyo tamaño es un hiperparámetro. La concatenación de estos vectores conforma la matriz de *embedding*, donde su cantidad de filas corresponde a la suma del tamaño del vocabulario más la cantidad reservada para los *out-of-vocabulary* (vocab_size + num_oov_buckets), y la cantidad de columnas a las dimensiones de los vectores (*embed_size*). Al igual que una matriz de pesos, ésta se inicializa de forma aleatoria, y actualiza sus valores para cada *step* de entrenamiento.\n",
    "\n",
    "  *   ¿Cuál es la ventaja de utilizar una capa de *embedding*? (Ver la sección *Encoding Categorical Features Using Embeddings* del capítulo 13 del libro.)\n",
    "  *   Visualizar una representación del espacio de *embedding* utilizando *Comet*.\n",
    "\n",
    "  Para este último punto se recomienda seguir el siguiente ejemplo: [logging-embeddings](https://www.comet.ml/docs/user-interface/embeddings/#logging-embeddings). Tener en cuenta que el parámetro *labels* de la función tiene que ser un *array* de *strings*, por lo cual si las palabras están codificadas con *utf-8* es necesario decodificarlas: \n",
    "\n",
    "\n",
    "```\n",
    "decoder = np.vectorize(lambda x: x.decode('UTF-8'))\n",
    "words_dec = decoder(words.numpy())\n",
    " ```\n",
    "\n",
    "*   Para su mejor modelo: visualizar a qué distancias se encuentran las palabras unas de otras tanto en la representación a baja dimensión como en el espacio de *embedding*. Sobre todo probar con adjetivos positivos (*wonderful*, *excellent*, etc.) y negativos (*ugly*, *boring*, etc.) comparando los resultados. ¿Qué logra observar?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIcsyeu_ZgKb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfSujuskbZog"
   },
   "source": [
    "# Parte 3: *Embedding Preentrenado*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4y9GduFbiZD"
   },
   "source": [
    "Una de las técnicas para mejorar el desempeño en este tipo de problemas es utilizar *embeddings* ya entrenados. \n",
    "Siguiendo el ejemplo [Using pre-trained word embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACU7xlN8bqzP"
   },
   "source": [
    "*   Descargar el *embedding* preentrenado [GloVE](https://nlp.stanford.edu/projects/glove/) que aparece en la sección *Load pre-trained word embeddings*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwbBgExIkGKR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9ARmJXcbyhU"
   },
   "source": [
    "* Preparar la nueva matriz de *embedding*. ¿Cuántas palabras del conjunto de entrenamiento se encuentran en el vocabulario de GloVE? ¿Cuántas no?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwyaRlGSkFvu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipkjXoNecR2_"
   },
   "source": [
    "* Entrenar el modelo con la nueva matriz de *embedding* de manera que los valores de ésta se mantengan fijos (ver parámetro en la capa de *embedding*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93_FhVyYkIiF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwqxdPALEyJ4"
   },
   "source": [
    "* Continue el entrenamiento ahora con el *embedding* entrenable. Modifique el *learning rate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtlzXIoUkMwh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0_RrsTZsaKM"
   },
   "source": [
    "*   Comparar con los modelos anteriores en cuanto al desempeño, la cantidad de parámetros y el tiempo de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elM29fxGkaIk"
   },
   "source": [
    "Respuesta: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af_D4NTCsezj"
   },
   "source": [
    "*   Visualizar cómo es el espacio de *embedding*. ¿Qué diferencias puede observar con respecto al anterior?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tsZq6m2FMjv"
   },
   "source": [
    "Respuesta: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnyfpJny_Eq-"
   },
   "source": [
    "# Parte 4: Mejoras en los Modelos (*Opcional*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luAwt0NIHQEO"
   },
   "source": [
    "Se sugieren algunas líneas que podrían mejorar los desempeños obtenidos en las partes anteriores: \n",
    "\n",
    "* Variar la cantidad de caracteres que se mantienen de la reseña así como el tamaño del vocabulario con el que se entrena. \n",
    "* Modificar la función de preprocesado.\n",
    "* Modificar las neuronas recurrentes. \n",
    "\n",
    "Piense por qué tiene sentido explorar estas estrategias y pruebe alguna que considere relevante. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HvLX8F1lDdH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90O_LXGExzWp"
   },
   "source": [
    "# Parte 5: Desarrollo de una aplicación web (*Opcional, no evaluado*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAIXZ-_DNeN7"
   },
   "source": [
    "En esta parte veremos cómo generar una aplicación web sencilla que permita mostrar el funcionamiento de un modelo que hayamos entrenado. Para ello utilizaremos la biblioteca [Streamit](https://streamlit.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:41:26.128144Z",
     "start_time": "2022-06-07T13:41:26.119600Z"
    },
    "id": "vOXeXXSUpM8o"
   },
   "source": [
    "## Streamlit\n",
    "Streamlit es una biblioteca de código abierto escrita en Python que permite crear y compartir aplicaciones web que usan algoritmos de aprendizaje automático. En la [documentación](https://docs.streamlit.io/) encontrará información sobre cómo instalar y crear aplicaciones utilizando la biblioteca. El flujo de trabajo básico consta de los siguientes pasos:   \n",
    "\n",
    "1. Instalación   \n",
    "2. Desarrollo de la aplicación   \n",
    "3. Despliegue de la aplicación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:42:20.285367Z",
     "start_time": "2022-06-07T13:42:20.280067Z"
    },
    "heading_collapsed": true,
    "id": "c4yAOe-apM8o"
   },
   "source": [
    "### Instalación\n",
    "\n",
    "En la mayoría de los casos, la biblioteca debería quedar instalada luego de crear un ambiente virtual (por ejemplo de conda) y hacer:   \n",
    "\n",
    "`pip install streamlit`  \n",
    "\n",
    "Puede verificar que la instalación sea correcta haciendo:    \n",
    "\n",
    "`streamlit hello`\n",
    "\n",
    "Puede ver los detalles de instalación en https://docs.streamlit.io/library/get-started/installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T13:43:13.534243Z",
     "start_time": "2022-06-07T13:43:13.529929Z"
    },
    "id": "FzA7GfkGpM8p"
   },
   "source": [
    "### Desarrollo de la aplicación  \n",
    "\n",
    "Se sugiere desarrollar la aplicación partiendo de un ejemplo que clasifica críticas de cine utilizando un modelo entrenado con las técnicas vistas en el Taller 2. Dicho ejemplo puede verse en funcionamiento [acá](https://share.streamlit.io/taa-fing/taa-2022/main/apps/movie_review_app/movie_review_app.py). La siguiente celda descarga el código fuente y lo descomprime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-07T16:35:48.707Z"
    },
    "id": "nXasOtuYpM8p"
   },
   "outputs": [],
   "source": [
    "!wget iie.fing.edu.uy/~carbajal/movie_review/apps.zip\n",
    "!unzip apps.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06snD_aCpM8p"
   },
   "source": [
    "Copie el archivo *movie_review_app.py*, modifique el nombre, y realice las siguientes modificaciones (además de las de diseño que crea conveniente):   \n",
    "\n",
    "**Cambio de Modelo:**  Para hacer inferencia fuera de este *Notebook* será necesario contar con el vocabulario y el modelo entrenado. El modelo se puede guardar con alguna de las técnicas vistas en el curso. Para el vocabulario, existen distintas formas, se otorga un código que almacena las palabras del diccionario en un archivo de *numpy*. Con estas palabras luego será necesario generar la *Tabla* del vocabulario nuevamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0fFPa1CP1U3"
   },
   "outputs": [],
   "source": [
    "# PATH a la dirección donde se almacenan las palabras del vocabulario. \n",
    "path_to_words = '/content/.../words.npy'\n",
    "\n",
    "# Guardo las palabras \n",
    "np.save(path_to_words, words.numpy(), allow_pickle=True)   \n",
    "\n",
    "#Levanto las palabras\n",
    "words = np.load(path_to_words, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYAdJltN6iWw"
   },
   "source": [
    "**Modificación del pipeline de inferencia:**  El objetivo es generar una función que a partir de una única reseña prediga si la reseña es *positiva*(1) o *negativa*(0). Para ello se brinda una función a completar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWkkHf_67KPS"
   },
   "outputs": [],
   "source": [
    "# Pipeline funtion to make inference \n",
    "def pipeline_inference (review, words, model): \n",
    "\n",
    "  '''\n",
    "  Función que prepara una review aislada y hace inferencia con el modelo. \n",
    "\n",
    "  Entradas: \n",
    "    review: String con la review a hacer inferencia\n",
    "    words: Arreglo de numpy que contiene las palabras del vocabulario\n",
    "    model: Modelo entrenado con el que se realiza inferencia\n",
    "\n",
    "  Salida: \n",
    "\n",
    "    pred: Predicción del modelo\n",
    "\n",
    "  '''\n",
    "  \n",
    "  # Parametros que haya utiliazado en el preprocesado y en la creación del vocabulario\n",
    "  n_characters= ?\n",
    "  vocab_size= ? \n",
    "  num_oov_buckets= ?\n",
    "\n",
    "  # Código para crear la tabla del vocabulario a partir de las palabras del diccionario\n",
    "  # ...\n",
    "  \n",
    "  # Código para codificar la reseña ingresada\n",
    "  # ...\n",
    "\n",
    "  # Código para convertir la reseña codificada a un tensor. Se sugiere utilizar tf.convert_to_tensor(). \n",
    "  # ...\n",
    "\n",
    "  # Cadena de Preprocesado de la reseña tal como lo hizo en el entrenamiento.\n",
    "  # ...\n",
    "\n",
    "  # Códifcar la reseña utilizando la tabla del vocabulario\n",
    "  # ...\n",
    "\n",
    "  # Agregar una dimension al tensor correspondiente al batch. Se sugiere utilizar la función tf.expand_dims()\n",
    "  # ...\n",
    "\n",
    "  # Código para hacer inferencia con el modelo\n",
    "  # ...\n",
    "  \n",
    "  return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_2uml4vSIvi"
   },
   "source": [
    "* Las siguientes celdas prueban la función. Verifique que funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qN9RStj0DQD_"
   },
   "outputs": [],
   "source": [
    "# Levanto el modelo\n",
    "checkpoint_filepath = '/content/...'\n",
    "\n",
    "model_loaded = keras.models.load_model(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePLSyMO1BMuX",
    "outputId": "9ca04dad-38eb-4ae9-a9a3-057c283b3887"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de Reseña Negativa\n",
    "review = 'This movie is really boring. I do not recommend it.'\n",
    "\n",
    "pred = pipeline_inference(review, words, model_loaded)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmV5G1WRTDpi",
    "outputId": "8708fbac-c8c7-4c3e-98f6-5d4d6ece49c8"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de Reseña Positiva\n",
    "review = 'This movie is wonderful. I love it.'\n",
    "\n",
    "pred = pipeline_inference(review, words, model_loaded)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVkAQmBHpM8r"
   },
   "source": [
    "### Correr la aplicación localmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1B4wy-_WpM8r"
   },
   "source": [
    "Una vez realizadas las modificaciones en el archivo principal, cree un directorio donde guardar los archivos de su aplicación. Guarde allí el modelo, el archivo tipo *numpy* con las palabras y su archivo principal (Ej. *movie_review_app.py*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Xm1pGUmbV6_"
   },
   "source": [
    "* Una vez modificado el código, puede probarlo localmente. Para ello ejecute el siguiente comando, sustituyendo *movie_review_app.py* por el nombre de su archivo principal.\n",
    "\n",
    "\n",
    "`!streamlit run apps/movie_review_app/movie_review_app.py`    \n",
    "\n",
    "* También puede correrlo de forma local desde Colab con el comando: \n",
    "\n",
    "`!streamlit run apps/movie_review_app/movie_review_app.py & npx localtunnel --port 80`    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoVErC2rpM8r"
   },
   "source": [
    "### Despliegue de la aplicación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKXAqZA4uAYA"
   },
   "source": [
    "Una vez que la app fue desarrollada es posible compartirla para que otros puedan probarla. Para ello es necesario:  \n",
    "\n",
    "1. Contar con una cuenta de [Stramlit Cloud](https://docs.streamlit.io/streamlit-cloud/get-started#sign-up-for-streamlit-cloud) y un repositorio de GitHub donde almacenar el código. \n",
    "2.  Subir al repositorio  el código y los datos necesarios para correrlo. \n",
    "3. [Conectar la cuenta de Streamlit Cloud con la del repositorio](https://docs.streamlit.io/streamlit-cloud/get-started#connect-your-github-account).     \n",
    "4. [Publicar la app](https://docs.streamlit.io/streamlit-cloud/get-started/deploy-an-app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN0y6d6rvO_u"
   },
   "source": [
    "*Comentario:* Puede que sea necesario agregar en el repositorio un archivo *requirements.txt* donde deba especificar las librerías utilizas en el archivo *main.py*."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rY6vz2Ekj8ig"
   ],
   "name": "taller9_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
